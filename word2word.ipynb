{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2word.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "X4rGbM3Attqi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NlicDpPoq_59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word-level review generation\n",
        "\n",
        "This part of our project utilizes word embedding to realize word-level review generation. Due to the large vocabulary size (typically tens of thousand) and limited computation resources, only 6000 most frequent words are selected. The embedded output is then fed as the input of a stacked two-layer gated recurrent units (GRU). Finally, the output is linearly projected to word space and yields a softmax probability.\n",
        "\n",
        "To improve the model performance and training speed, within GRU, a dropout probability of 0.2 is employed to all reset and input gates. A dropout probability of 0.5 is employed between the word embedding layer and the GRU, and between the GRU and the fully-connected output layer. \n",
        "\n",
        "**Sample generated texts:**\n",
        "\n",
        "- If you're looking for a small place to take a good time at the end of the night. I would recommend this place to anyone to try the <unk\\>, and you won't be disappointed. Weeknight, and the food is great.\n",
        "- My husband and I were looking for a little <unk\\>. The pizza was delicious, the shrimp was good and the chicken was good! My friend had the chicken and the shrimp and it was delicious. The chips were great and the food was amazing!\n",
        "- This is a great place to eat. I've had a few people here. The place is very nice and the service is exceptional. I would recommend the food and food.\n",
        "- Very good, the food was delicious. I had a great meal and the food was great. I'm so glad I had the same thing that I had. I'm not sure if I was going to get a drink.\n",
        "- A few years ago, and I was really excited to try for my first time. I'm glad I would be going back to the place. The food was good, the food was good, but the food was pretty good.\n",
        "- I was told that the other employee came out and said they were. We sat in the front desk for our table. They brought us the food and the waiter was very friendly and nice.\n",
        " \n",
        " (Nothing changed except for formatting)\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "n6nPdUjzuyus",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "78YPmCDZaBPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0b6bec2-9735-4603-f370-bfaf845bf3f6"
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import layers\n",
        "from keras.layers import Dense, LSTM, GRU, Dropout, TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dv02cDSBnQxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_text(filename):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        sentences = f.readlines()\n",
        "\n",
        "    sentences = [sentence.lower() for sentence in sentences]\n",
        "    sentences = [re.sub(r'([\\.\\,\\?\\!\\']+)', r' \\1 ', sentence) for sentence in sentences]\n",
        "    sentences = [re.sub(r'[\"#\\$%&()\\*\\+\\-/:;\\<\\=\\>@\\\\^_`\\{\\|\\}~\\t\\n]', '', sentence) for sentence in sentences]\n",
        "    sentences = [re.sub(r'\\s+', ' ', sentence) for sentence in sentences]\n",
        "    sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "    print('{} sentences loaded. Sample:\"\"'.format(len(sentences)))\n",
        "    print(' '.join(random.choice(sentences)))\n",
        "    \n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LuKC1hB4tmNS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Load pretrained word embedding"
      ]
    },
    {
      "metadata": {
        "id": "smjOJ6kQaNzP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_model = gensim.models.Word2Vec.load(\"word2vec2.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4rGbM3Attqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training of the word embedding (optional)\n",
        "\n",
        "The codes below loads 100000 lines of texts and use skip-gram and CBOW to train a word2vec model."
      ]
    },
    {
      "metadata": {
        "id": "dgaw8ECGDFYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a9b2fa5d-3598-41cc-f800-9cec9ce9091f"
      },
      "cell_type": "code",
      "source": [
        "sentences = load_text('input_100000.txt')\n",
        "sentences = [sentence for sentence in sentences if len(sentence) >= 2]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100068 sentences loaded. Sample:\"\"\n",
            "nail came out really cute , but they were extremely shorthanded . i went in with 2 other people and after over 2 hours in the place my nails and toes were finished while the other 2 were still waiting to be seated .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82xdkf-Cvb5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f002b238-2e25-498a-c36c-8293321054ae"
      },
      "cell_type": "code",
      "source": [
        "# Pretrain word embedding\n",
        "\n",
        "embedding_size = 128\n",
        "print('\\nTraining word2vec...')\n",
        "word_model = gensim.models.Word2Vec(sentences, size=embedding_size, max_final_vocab=6000, window=5, iter=1000)\n",
        "pretrained_weights = word_model.wv.vectors\n",
        "vocab_size, embedding_size = pretrained_weights.shape\n",
        "print('vocab_size: {} embedding size: {}'.format(vocab_size, embedding_size))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training word2vec...\n",
            "vocab_size: 5959 embedding size: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2tvoM_T2wmbL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_model.save(\"word2vec2.model\")\n",
        "# word_model.train(sentences, total_examples=word_model.corpus_count, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q9Ice_jwtKnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f2ad6df6-c678-4785-a737-76dfb497c2b1"
      },
      "cell_type": "code",
      "source": [
        "for word in ['sushi', 'beef', 'toppings', 'green']:\n",
        "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
        "  print('  %s -> %s' % (word, most_similar))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  sushi -> sashimi (0.67), pho (0.59), food (0.58), nigiri (0.57), ramen (0.54), poke (0.48), seafood (0.48), dimsum (0.45)\n",
            "  beef -> pork (0.62), chicken (0.57), steak (0.52), shrimp (0.52), meat (0.50), lamb (0.48), soup (0.48), veal (0.46)\n",
            "  toppings -> ingredients (0.61), flavors (0.60), veggies (0.58), sides (0.57), meats (0.55), sauces (0.52), choices (0.51), topping (0.50)\n",
            "  green -> lemon (0.48), black (0.47), red (0.45), pineapple (0.44), fritters (0.43), roasted (0.42), cayenne (0.42), bean (0.42)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xkM7dd-ruFE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare data"
      ]
    },
    {
      "metadata": {
        "id": "Pj1TzE05vfKX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When tokenization, reserves index 0 for padding and index 1 for unk."
      ]
    },
    {
      "metadata": {
        "id": "Ps2r8ePo6_oK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 0 for mask\n",
        "pretrained_weights_one = np.insert(pretrained_weights, 0, 0,axis=0)\n",
        "\n",
        "# 1 for <UNK>\n",
        "embedding_w = np.insert(pretrained_weights_one, 0, 0, axis=0)\n",
        "embedding_w[1:] = np.mean(pretrained_weights, axis=0)\n",
        "\n",
        "vocab_size, embedding_size = embedding_w.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmkw7hqs1NG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pretrained_weights = word_model.wv.vectors\n",
        "vocab_size, embedding_size = pretrained_weights.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRBuXQ2g_XLN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word2idx(word):\n",
        "    if word == '<UNK>' or word not in word_model.wv.vocab:\n",
        "        return 1\n",
        "    return word_model.wv.vocab[word].index + 2\n",
        "def idx2word(idx):\n",
        "    if idx == 1:\n",
        "        return '<UNK>'\n",
        "    return word_model.wv.index2word[idx - 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cs5Jy5X7uXJf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text Preparation and Tokenization\n",
        "\n",
        "The following codes tokenize text in `input_100000.txt`.\n",
        "\n",
        "Besides, it breaks lines that are longer than `max_len` into smaller ones. As a result, there will be 275k lines (instead of 100k) in the dataset. \n",
        "\n",
        "All lines that are too short (less than `min_len`) are pruned."
      ]
    },
    {
      "metadata": {
        "id": "6zCK1CoAah2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fc1c43f6-a70c-42d1-a9a5-eef8cb005b3a"
      },
      "cell_type": "code",
      "source": [
        "# Dataset Prep for training\n",
        "\n",
        "max_len = 50\n",
        "min_len = 10\n",
        "\n",
        "sentences = load_text('input_100000.txt')\n",
        "sentences = [sentence for sentence in sentences if len(sentence) >= min_len]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100068 sentences loaded. Sample:\"\"\n",
            "always manage a good time here . good steam and hot tub . the dark areas are fun and the sling can support a great amount of weight . nice guys and the roof is a sunny refuge . i will be back .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iM-9IN0-eAp3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cb73202-62c6-4855-8931-98c3f2a82a5c"
      },
      "cell_type": "code",
      "source": [
        "tokenized = [[word2idx(w) for w in sentence] for sentence in sentences]\n",
        "\n",
        "tokenized_m = []\n",
        "length = max_len + 1 # for x and y\n",
        "for sentence in tokenized:\n",
        "    while len(sentence) > length:\n",
        "        tokenized_m.append(sentence[:length])\n",
        "        sentence = sentence[length:]\n",
        "    if len(sentence) >= min_len:\n",
        "        tokenized_m.append(sentence + [0] * (length  - len(sentence)))\n",
        "\n",
        "tokenized_m = np.array(tokenized_m)\n",
        "print(tokenized_m.shape)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(275288, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fm400dn7v0mH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Dataset\n",
        "\n",
        "Each line in the target data set `y` is identical to that in the target data set `x` except for that `y` is shifted 1 index forward, i.e. `train_y[:, i, :]` is equal to `train_x[:, i+1, :]` (not considering the first and last element).\n",
        "\n",
        "During training, the cross entropy loss for **every** word (rather than merely the last one) in `y` will be taken into consideration."
      ]
    },
    {
      "metadata": {
        "id": "kKoKSVOpw6sO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b8bfef02-81c7-4c3c-f44b-652a30fedb74"
      },
      "cell_type": "code",
      "source": [
        "tokenized_len = tokenized_m.shape[1]\n",
        "train_x = tokenized_m[:, :tokenized_len-1]\n",
        "train_y = tokenized_m[:, 1:]\n",
        "print('train_x shape:', train_x.shape)\n",
        "print('train_y shape:', train_y.shape)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x shape: (275288, 50)\n",
            "train_y shape: (275288, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Or-tG20mxL-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ]
    },
    {
      "metadata": {
        "id": "EYnldElJt0sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f65dcca9-6537-42ad-da15-cd9f6d1949f8"
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "max_words = vocab_size\n",
        "\n",
        "# build the model: two layer LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_len,\n",
        "                    weights=[embedding_w],\n",
        "                    mask_zero=True,\n",
        "#                     trainable=False,\n",
        "                   ))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(GRU(embedding_size, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(GRU(embedding_size, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "# model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 128)           763008    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 50, 128)           98688     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 50, 128)           98688     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 50, 5961)          768969    \n",
            "=================================================================\n",
            "Total params: 1,729,353\n",
            "Trainable params: 1,729,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vBG4cEEL2dOH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0, exclude_unk=False):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    if exclude_unk:\n",
        "        for i in range(3):\n",
        "            probas = np.random.multinomial(1, preds, 1)\n",
        "            result = np.argmax(probas)\n",
        "            if result > 1:\n",
        "                break\n",
        "    else:\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        result = np.argmax(probas) \n",
        "        \n",
        "    return result\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    for diversity in [0.2, 0.6]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence_list = [[1]]\n",
        "        for _ in range(2):\n",
        "            sentence_list.append([random.randrange(2, 250)])\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            generated = sentence\n",
        "\n",
        "            for i in range(20):\n",
        "                x_pred = np.zeros((1, max_len))\n",
        "                for t, word in enumerate(sentence):\n",
        "                    x_pred[0, t] = word\n",
        "\n",
        "                preds = model.predict(np.array(x_pred), verbose=0)\n",
        "                next_index = sample(preds[-1, i], diversity)\n",
        "\n",
        "                sentence.append(next_index)\n",
        "\n",
        "            print(' '.join([idx2word(w) for w in sentence]))\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "save_callback = ModelCheckpoint(os.path.join('model',\n",
        "                                             \"weights.{epoch:d}-{loss:.2f}.hdf5\"),\n",
        "                                monitor='loss', period=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0l_kq_Cqxiu8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_x, np.expand_dims(train_y, -1), batch_size=1024, epochs=5,\n",
        "          callbacks=[\n",
        "                     print_callback, \n",
        "                     save_callback,\n",
        "                    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XoxiYBDjxmSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Generation"
      ]
    },
    {
      "metadata": {
        "id": "Iwqwfn9B0DkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5870932f-aef1-4279-f2a9-1f4e647256d6"
      },
      "cell_type": "code",
      "source": [
        "! ls model"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights.10-6.00.hdf5  weights.25-4.51.hdf5  weights.5-6.08.hdf5\n",
            "weights.15-4.74.hdf5  weights.30-4.47.hdf5\n",
            "weights.20-4.58.hdf5  weights.35-4.44.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MX81lEOv0FJv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = load_model('model/weights.35-4.44.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eLf9G_arHgRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "ca43ea50-b3d7-4eae-ab17-2b92c41e599a"
      },
      "cell_type": "code",
      "source": [
        "generated = ''\n",
        "sentence_list = [['this'], ['my'], ['it'],\n",
        "                ['a'], ['many'], ['the'],\n",
        "                ['if'], ['so'], ['in'],\n",
        "                ['i'], ['we'], ['very']]\n",
        "\n",
        "for sentence in sentence_list:\n",
        "    generated = sentence\n",
        "\n",
        "    for i in range(max_len - len(sentence)):\n",
        "        x_pred = np.zeros((1, max_len))\n",
        "        for t, word in enumerate(sentence):\n",
        "            x_pred[0, t] = word2idx(word)\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)\n",
        "#         print(preds.shape)\n",
        "        next_index = sample(preds[-1, i], 0.5, True)\n",
        "#         next_index = np.argmax(preds[-1, i])\n",
        "        next_word = idx2word(next_index)\n",
        "\n",
        "        sentence.append(next_word)\n",
        "#     print(' '.join([idx2word(np.argmax(wv)) for wv in preds[-1,:20,:]]))\n",
        "    print(' '.join(sentence))\n",
        "    print(' ')"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this location is a shame , and i ' m not sure what we ' ve ever had . i love the <UNK> . i think it ' s a big fan of a <UNK> and one of the best restaurants in vegas . my favorite is that the <UNK>\n",
            " \n",
            "my husband and i have been wanting to try the chicken and the chicken . the rice is good , but the food is great ! i ' m a big fan of the fish and beef and the sweet potato salad . the food is fantastic and i definitely\n",
            " \n",
            "it was a bit pricey . the ambiance was nice , the staff was nice and helpful . we had a great experience . i ordered the <UNK> and a <UNK> and i loved the chicken . the salad was fresh and the chicken was outstanding . the chicken was\n",
            " \n",
            "a few times . the food is excellent and the service is excellent . i think it ' s a must try . weeknight and a friendly place . weeknight is not a lot of food , but it ' s a great place to hang out . weeknight ,\n",
            " \n",
            "many . the service was excellent , the food was great . i ' ve been going to the <UNK> <UNK> for a few times and i ' m a bit surprised with the menu . i got the fried noodles to the side of the <UNK> , and the\n",
            " \n",
            "the <UNK> of the <UNK> for the <UNK> of the <UNK> . i had a <UNK> , <UNK> , and <UNK> . i went with a <UNK> <UNK> and was able to get a few minutes to get it . i was surprised that the <UNK> was a little more\n",
            " \n",
            "if you ' re looking for a small place to take a good time at the end of the night . i would recommend this place to anyone to try the <UNK> , and you won ' t be disappointed . weeknight , and the food is great . they\n",
            " \n",
            "so i had to pay with the <UNK> . the first time i had a very nice time the staff was friendly and helpful . the food was excellent ! the food was excellent and the service was great . the food was so good , but it was a\n",
            " \n",
            "in a <UNK> . all of the other items are the best you are in the area . the food is great and the food is very good . i ' ve had the best bbq in town and the food is great . the service is always good ,\n",
            " \n",
            "i was told that the menu was the same . i ordered the chicken tacos with a side of rice which was not the best . the food was good , and the chips were good . the chicken was good , the fries were good and the meat was\n",
            " \n",
            "we ordered a <UNK> roll and the salad was great . the food was a bit pricey , but i ' m not sure if the food was delicious . i love the food . the food was great , and the food was good . the menu was pretty\n",
            " \n",
            "very nice atmosphere and a great place to eat so i ' m not sure what you can ' t have to sit on . i ' ve been here a couple of times , and i ' m so glad i was ordering . we had a great time\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-fPenti1yj9W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}